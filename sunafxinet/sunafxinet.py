import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

from demucs.htdemucs import HTDemucs
from afxcdt import AFXCDT
from hafx import HAfx

class SunAFXiNet(HTDemucs):
    """
    HTDemucsã‚’ç¶™æ‰¿ã—ã€AFXé™¤å»ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®šã®ãŸã‚ã«æ”¹é€ ã—ãŸå®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã€‚
    """
    def __init__(self, hdemucs_config, num_effects, param_dims):
        # ä¿¡å·å¾©å…ƒã‚¿ã‚¹ã‚¯ãªã®ã§ã€å‡ºåŠ›ã‚½ãƒ¼ã‚¹ã¯1ã¤ã«è¨­å®š
        hdemucs_config['sources'] = ['restored']
        
        # HTDemucsã®åˆæœŸåŒ–
        super().__init__(**hdemucs_config)
        
        # Transformerã®æ¬¡å…ƒæ•°ã‚’å–å¾—
        # ã€ä¿®æ­£ç‚¹ã€‘self.growthã‚„self.depthã¯HTDemucsã®__init__ã§å±æ€§ã¨ã—ã¦ä¿å­˜ã•ã‚Œãªã„ãŸã‚ã€
        # ã‚³ãƒ³ãƒ•ã‚£ã‚°è¾æ›¸ã‹ã‚‰ç›´æ¥å€¤ã‚’å–å¾—ã™ã‚‹
        growth = hdemucs_config.get('growth', 2) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã‚‹
        depth = hdemucs_config.get('depth', 4)   # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯å…ƒã®ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã‚‹
        transformer_channels = self.channels * (growth ** (depth - 1))
        
        if self.bottom_channels:
            transformer_channels = self.bottom_channels
            
        # HAfxãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–
        # print(f"âœ… Initializing transformer with dimension (d_model): {transformer_channels}")
        self.hafx = HAfx(
            input_dim=transformer_channels, 
            num_effects=num_effects, 
            param_dims=param_dims
        )
        
        # AFXCDTãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åˆæœŸåŒ–
        # print("start AFXCDT initialization")
        if self.crosstransformer is not None:
            # CrossTransformerEncoderã«è¨­å®šã‚’å–å¾—ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ãŒãªã„ã¨ä»®å®šã—ã€
            # __init__ã®å¼•æ•°ã‹ã‚‰æ‰‹å‹•ã§è¨­å®šã‚’å†æ§‹ç¯‰
            cdt_config = {k.replace('t_', ''): v for k, v in hdemucs_config.items() if k.startswith('t_')}
            # print(f"config[dim] = {transformer_channels}")
            cdt_config['dim'] = transformer_channels
            if 'layers' in cdt_config: cdt_config['num_layers'] = cdt_config.pop('layers')
            if 'heads' in cdt_config: cdt_config['num_heads'] = cdt_config.pop('heads')
            # print(cdt_config)
            self.afx_cdt = AFXCDT(cdt_config, num_effects)
            # ã€ä¿®æ­£ã€‘å…ƒã®crosstransformerã‚’ç„¡åŠ¹åŒ–ã—ã€æ„å›³ã—ãªã„å‘¼ã³å‡ºã—ã‚’é˜²ã
            self.crosstransformer = None
        else:
            self.afx_cdt = None

    def forward(self, mix, afx_type_condition=None):
        """
        HTDemucs.forwardã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã€AFXæ¨å®šã¨æ¡ä»¶ä»˜ã‘ã‚’çµ„ã¿è¾¼ã‚€ã€‚

        Args:
            mix (torch.Tensor): å…¥åŠ›ã‚¦ã‚§ãƒƒãƒˆä¿¡å· (B, C, L)ã€‚
            afx_type_condition (torch.Tensor, optional):
                å­¦ç¿’ã®ç¬¬1æ®µéšã§ä½¿ç”¨ã™ã‚‹ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã®one-hotãƒ™ã‚¯ãƒˆãƒ« (B, num_effects)ã€‚
                Noneã®å ´åˆã¯hafxã®æ¨å®šå€¤ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
        Returns:
            tuple: (s_hat, type_logits, param_predictions)
                s_hat (torch.Tensor): å¾©å…ƒã•ã‚ŒãŸä¿¡å· (B, 1, C, L)ã€‚
                type_logits (torch.Tensor): (B, num_effects)
                param_predictions (dict): {'Distortion': (B, p_dim),...}
        """
        # ===== HTDemucs.forward() ã‹ã‚‰ã®ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€éƒ¨åˆ†ï¼‰=====
        # print("start SunAFXiNet Encoder")
        length = mix.shape[-1]
        length_pre_pad = None
        if self.use_train_segment:
          if self.training:
            self.segment = length / self.samplerate
          else:
            training_length = int(self.segment * self.samplerate)
            if mix.shape[-1] < training_length:
                length_pre_pad = mix.shape[-1]
                mix = F.pad(mix, (0, training_length - length_pre_pad))
        z = self._spec(mix)
        mag = self._magnitude(z).to(mix.device)
        x = mag
        B, C, Fq, T = x.shape
        # unlike previous Demucs, we always normalize because it is easier.
        mean = x.mean(dim=(1, 2, 3), keepdim=True)
        std = x.std(dim=(1, 2, 3), keepdim=True)
        x = (x - mean) / (1e-5 + std)
        # x will be the freq. branch input.

        # Prepare the time branch input.
        xt = mix
        meant = xt.mean(dim=(1, 2), keepdim=True)
        stdt = xt.std(dim=(1, 2), keepdim=True)
        xt = (xt - meant) / (1e-5 + stdt)

        # okay, this is a giant mess I know...
        saved = []  # skip connections, freq.
        saved_t = []  # skip connections, time.
        lengths = []  # saved lengths to properly remove padding, freq branch.
        lengths_t = []  # saved lengths for time branch.

        for idx, encode in enumerate(self.encoder):
            lengths.append(x.shape[-1])
            inject = None
            if idx < len(self.tencoder):
                lengths_t.append(xt.shape[-1])
                tenc = self.tencoder[idx]
                xt = tenc(xt)
                if not tenc.empty:
                    saved_t.append(xt)
                else:
                    inject = xt
            x = encode(x, inject)
            if idx == 0 and self.freq_emb is not None:
                frs = torch.arange(x.shape[-2], device=x.device)
                emb = self.freq_emb(frs).t()[None, :, :, None].expand_as(x)
                x = x + self.freq_emb_scale * emb
            saved.append(x)

        # ===== ã“ã“ã‹ã‚‰ãŒSunAFXiNetã®ä»‹å…¥éƒ¨åˆ† =====

        # 1. hafxã§AFXã‚’æ¨å®š (Transformerã«å…¥ã‚‹ç›´å‰ã®å‘¨æ³¢æ•°è¡¨ç¾ `x` ã‚’ä½¿ç”¨)
        # print("start HAfx inference")
        type_logits, param_predictions = self.hafx(x)

        # 2. æ¡ä»¶ä»˜ã‘ç”¨ã®one-hotãƒ™ã‚¯ãƒˆãƒ«ã‚’æ±ºå®š
        # print("decide one-hot vector")
        if afx_type_condition is not None:
            # å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¸1: ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒˆã‚¥ãƒ«ãƒ¼ã‚¹ã§æ¡ä»¶ä»˜ã‘
            condition_one_hot = afx_type_condition
        else:
            # å­¦ç¿’ã‚¹ãƒ†ãƒ¼ã‚¸2 & æ¨è«–: hafxã®æ¨å®šå€¤ã§æ¡ä»¶ä»˜ã‘
            pred_indices = torch.argmax(type_logits, dim=1)
            condition_one_hot = F.one_hot(pred_indices, num_classes=self.hafx.num_effects).float()

        # 3. AFXCDT (crosstransformer) ã‚’å‘¼ã³å‡ºã—
        # print("start AFXCDT inference")
        if self.afx_cdt is not None: # self.crosstransformerã¯AFXCDTã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            if self.bottom_channels:
                b, c, f, t = x.shape
                x = rearrange(self.channel_upsampler(rearrange(x, "b c f t -> b c (f t)")), "b c (f t) -> b c f t", f=f)
                xt = self.channel_upsampler_t(xt)
            
            # AFXCDTã«æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ ã§æ¸¡ã™
            # print(f"ğŸš¨ Shape of 'x' before afx_cdt: {x.shape}")
            # print(f"ğŸš¨ Shape of 'xt' before afx_cdt: {xt.shape}")
            x, xt = self.afx_cdt(x, xt, effect_type_one_hot=condition_one_hot)
            
            if self.bottom_channels:
                b, c, f, t = x.shape
                x = rearrange(self.channel_downsampler(rearrange(x, "b c f t -> b c (f t)")), "b c (f t) -> b c f t", f=f)
                xt = self.channel_downsampler_t(xt)

        # ===== HTDemucs.forward() ã‹ã‚‰ã®ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ‡ã‚³ãƒ¼ãƒ€éƒ¨åˆ†ï¼‰=====
        # print("start SunAFXiNet Decoder")
        for idx, decode in enumerate(self.decoder):
            skip = saved.pop(-1)
            x, pre = decode(x, skip, lengths.pop(-1))
            offset = self.depth - len(self.tdecoder)
            if idx >= offset:
                tdec = self.tdecoder[idx - offset]
                length_t = lengths_t.pop(-1)
                if tdec.empty:
                    xt, _ = tdec(pre[:, :, 0], None, length_t)
                else:
                    xt, _ = tdec(xt, saved_t.pop(-1), length_t)
        
        S = len(self.sources)
        x = x.view(B, S, -1, Fq, T) * std[:, None] + mean[:, None]
        zout = self._mask(z, x)
        s_hat_spec = self._ispec(zout, length)
        xt = (xt.view(B, S, -1, length) * stdt[:, None] + meant[:, None])
        s_hat = xt + s_hat_spec

        if length_pre_pad:
            s_hat = s_hat[..., :length_pre_pad]

        return s_hat, type_logits, param_predictions
